{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1118201e-6bc9-491f-a181-5afdb04c575e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Dataset"
    }
   },
   "outputs": [],
   "source": [
    "mount_path = '/mnt/vision-test'\n",
    "normal_data = f'{mount_path}/gold_dataset_normal'\n",
    "abnormal_data = f'{mount_path}/gold_dataset_abnormal'\n",
    "\n",
    "df_augmented = (\n",
    "    spark.read.parquet(normal_data)\n",
    "    .union(spark.read.parquet(abnormal_data)\n",
    "           )\n",
    ")\n",
    "\n",
    "# display(df_augmented)\n",
    "\n",
    "dataset_name = \"training_dataset_augemented\"\n",
    "df_augmented.write.mode(\"overwrite\").format(\"parquet\").save(f\"{mount_path}/{dataset_name}\")\n",
    "df_augmented.write.mode(\"overwrite\").format(\"delta\").saveAsTable(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40ebbff-2783-4f55-af9e-5fdc041daa89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 labels found in the dataset\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'path', 'label', 'image'],\n",
       "    num_rows: 960\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'name': Value(dtype='string', id=None),\n",
       " 'path': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int32', id=None),\n",
       " 'image': Value(dtype='binary', id=None)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df = spark.read.table(dataset_name)\n",
    "dataset = Dataset.from_spark(df)\n",
    "\n",
    "num_labels = len(dataset.unique('label'))\n",
    "print(f\"{num_labels} labels found in the dataset\")\n",
    "display(dataset)\n",
    "display(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853102c3-378c-4ae9-9dfa-842b0f08b1fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment Set Up"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Add device selection logic\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3c13c6-a0a2-45cd-a085-fe89ce965328",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Building Deep Learning"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375818416beb4b3282eb23146e63d2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e7c56dd905492aa0430ca5c134c666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07fb19dfcc246b784b2aa6249d6e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification\n",
    ")\n",
    "\n",
    "\n",
    "class WrappedViT(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.model(x)\n",
    "        return outputs.logits # only return logits (a single tensor)\n",
    "\n",
    "\n",
    "# Specify a pre-trained model\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    use_fast = True,\n",
    "    do_resize= True,\n",
    "    size = 224\n",
    "    )\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the model\n",
    "wrapped_model = WrappedViT(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3680a036-8644-45d6-9dad-f46f0274e0dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Classifier Weights"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)\n",
    "torch.nn.init.xavier_uniform_(model.classifier.weight)\n",
    "model.classifier.bias.data.fill_(0)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a0a0b9-5fbc-4524-adf7-d69d4f201e08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Input(Image)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8f0c68ccfe47f4a95391a16ab444d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a preprocessing function to handle binary inmage data\n",
    "\n",
    "def preprocess(example):\n",
    "    # Convert binary image data to PIL Image with RGB channel\n",
    "    image = Image.open(io.BytesIO(example['image'])).convert(\"RGB\")\n",
    "\n",
    "    # Process the image using the image processor\n",
    "    processed_image = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # [1,3,224,224] -> [3,224,224]\n",
    "    example['pixel_values'] = processed_image['pixel_values'].squeeze()\n",
    "    example['pixel_values'] = example['pixel_values'].to(device)\n",
    "    return example\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "# Set the format of dataset to PyTorch Tensors\n",
    "dataset.set_format(type='torch', columns=['pixel_values', 'label'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e92791f-db95-478d-a3e3-6ea4e55c891c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 132 abnormal, 636 normal\nEval dataset: 36 abnormal, 156 normal\n"
     ]
    }
   ],
   "source": [
    "train_ds_abnormal = len(train_dataset['label'][train_dataset['label'] == 0])\n",
    "train_ds_normal = len(train_dataset['label'][train_dataset['label'] == 1])\n",
    "eval_ds_abnormal = len(eval_dataset['label'][eval_dataset['label'] == 0])\n",
    "eval_ds_normal = len(eval_dataset['label'][eval_dataset['label'] == 1])\n",
    "\n",
    "print(f\"Training dataset: {train_ds_abnormal} abnormal, {train_ds_normal} normal\")\n",
    "print(f\"Eval dataset: {eval_ds_abnormal} abnormal, {eval_ds_normal} normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc3bfb6-27e2-418d-ac97-5482391b25a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training Set up"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "import os# Set environment vairables to avoid the warning\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "os.environ['MKL_NUM_THREADS'] ='16'\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/tmp/huggingface/{model_name}-finetuned-dog\",\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    ddp_find_unused_parameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3da556-f24c-498a-9d8a-91af33a39889",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation Metrics"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2fb53e459e4f0f8b52b2198ad6a98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc8aca7-e988-4eef-a4e4-76c327c327d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train(MLflow)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-15 17:33:06,804] [WARNING] [real_accelerator.py:194:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n[2025-09-15 17:33:06,816] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 05:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.395547</td>\n",
       "      <td>0.925816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b5d4d09c2646c68975021552c2a3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Start an MLflow run\n",
    "run_name = f\"vit-classification-wrapped-{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    early_stop = EarlyStoppingCallback(early_stopping_patience=5)\n",
    "    \n",
    "    # Initialize\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "\n",
    "\n",
    "    # Log training metrics\n",
    "    mlflow.log_metrics(train_result.metrics)\n",
    "\n",
    "    # Evaluate and log metrics\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    mlflow.log_metrics(eval_metrics)\n",
    "\n",
    "\n",
    "    # Get a sample input and prepare it for signature\n",
    "    sample_input = next(iter(eval_dataset))\n",
    "    input_tensor = sample_input['pixel_values'].unsqueeze(0) #[-1,3,224,224]\n",
    "\n",
    "    # Get model prediction for signature\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_output = model(input_tensor)\n",
    "    \n",
    "    # Convert to numpy arrays for MLflow\n",
    "    input_array = input_tensor.cpu().numpy()\n",
    "    output_array = sample_output.logits.cpu().numpy()\n",
    "\n",
    "    # Create signature\n",
    "    signature = infer_signature(input_array, output_array)\n",
    "\n",
    "    # Log requirements\n",
    "    reqs = mlflow.transformers.get_default_pip_requirements(model)\n",
    "\n",
    "    # Log the model with MLflow\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model = wrapped_model,\n",
    "        artifact_path = 'model',\n",
    "        signature = signature,\n",
    "        pip_requirements = reqs\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Log the input dataset for lineage tracking from table to model\n",
    "    src_dataset = mlflow.data.load_delta(\n",
    "        table_name = dataset_name\n",
    "    )\n",
    "    mlflow.log_input(src_dataset, context='Training-Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431191b0-9bc1-4df6-97a0-a873bbb41096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_HF_Deep_Learning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}