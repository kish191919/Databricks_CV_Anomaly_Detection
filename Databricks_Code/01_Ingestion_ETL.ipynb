{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf9b43c-a897-4216-b0c6-68dd03edc858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/yesmanki81@gmail.com/Databricks_CV_Anomaly_Detection/Databricks_Code/00_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc6d6fb-7252-462f-a8f9-5383cfeb246c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the mount point exists\n",
    "mount_path = \"/mnt/vision-test\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412d3fce-2fe4-4e8a-a59a-fcca79c880ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the directory\n",
    "# dbutils.fs.unmount(f\"/mnt/{mount_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed14f05-ebc2-418c-b6f1-957b5b32c694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9da478-1081-4c12-b547-8fccb31af0c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Image"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of FileInfo objects to a spark DataFrame\n",
    "def create_file_into_df(source_dir:str):\n",
    "\n",
    "    files = dbutils.fs.ls(f\"{mount_path}/{source_dir}\")\n",
    "\n",
    "    file_info_list = [\n",
    "        {\n",
    "            \"path\": file.path,\n",
    "            \"name\": file.name\n",
    "        }\n",
    "        for file in files\n",
    "        ]\n",
    "\n",
    "    return spark.createDataFrame(file_info_list)\n",
    "\n",
    "file_info_df = create_file_into_df(\"images\")\n",
    "display(file_info_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd5ff68-2753-42d8-8632-73f7d402e14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the label file\n",
    "label_files = dbutils.fs.ls(f\"{mount_path}/labels\")\n",
    "label_path = label_files[0].path\n",
    "\n",
    "label_df = spark.read.csv(label_path, header=True, inferSchema=True)\n",
    "label_df = label_df.withColumnRenamed(\"image_name\", \"name\")\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "label_df = label_df.withColumn(\"name\", concat(label_df[\"name\"], lit(\".jpg\")))\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e52cfc-dcc6-40fe-a8b1-ee6e48065f35",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757161712438}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the two DataFrames\n",
    "file_info_df = file_info_df.join(label_df, on=\"name\", how=\"left\")\n",
    "display(file_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013ccb53-ed77-407d-ae72-36abd33e3fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Spark에서는 분산되어있기 때문에 collect를 사용\n",
    "\n",
    "def display_image(df, num_images:int=5):\n",
    "    images = df.take(num_images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i, row in enumerate(images):\n",
    "        img_path = row.path.replace('dbfs:/','/dbfs/')\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1,num_images,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(row.name)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_image(file_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b07678-b894-4f62-b9fd-109e1d707977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test: Crop and resize"
    }
   },
   "outputs": [],
   "source": [
    "for row in file_info_df.collect():\n",
    "    img_path = row.path.replace('dbfs:/','/dbfs/')\n",
    "    img = Image.open(img_path)\n",
    "    width, height = img.size\n",
    "    print(f\"Image: {row.name}, size: {width} x {height}\")\n",
    "    new_size = min(width, height)\n",
    "\n",
    "    # Crop 순서: LL, UL, UR, LR\n",
    "    img = img.crop(((width-new_size)/2, (height-new_size)/2, (width+new_size)/2, (height+new_size)/2))\n",
    "    print(f\"{img.size = }\")\n",
    "\n",
    "    # Resize : 256 x 256\n",
    "    img = img.resize((256, 256), Image.NEAREST)\n",
    "    print(f\"{img.size = }\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19ebac9-58d2-46d2-839c-db9bac87d667",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pandas UDF"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def resize_image_udf(df_series: pd.Series) -> pd.Series:\n",
    "    def resize_image(path):\n",
    "        \"\"\"Resize image and serialize back as jpeg\"\"\"\n",
    "        # load image\n",
    "        img_path = path.replace('dbfs:/','/dbfs/')\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        new_size = min(width, height)\n",
    "        \n",
    "        # Crop: LL, UL, UR, LR\n",
    "        img = img.crop(((width-new_size)/2, (height-new_size)/2, (width+new_size)/2, (height+new_size)/2))\n",
    "\n",
    "        # Resize\n",
    "        img = img.resize((IMAGE_SIZE,IMAGE_SIZE), Image.NEAREST)\n",
    "\n",
    "        # Save back to jpg\n",
    "        output = io.BytesIO()\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "\n",
    "    return df_series.apply(resize_image)\n",
    "\n",
    "# Add the metadata to enable the image preview\n",
    "image_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\": \"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "df = (\n",
    "    file_info_df.withColumn(\"image\", resize_image_udf(\"path\")).withColumn('image',col('image').alias('image', metadata=image_meta))\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c244a14-ddef-4c84-a1ab-ec734674788e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format('parquet').save(f\"{mount_path}/images_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ae2f3d-eaf0-4a9d-9af0-3f5ed332d5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lit \n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def flip_image_horizontally_udf(df_series):\n",
    "    def flip_image(binary_content):\n",
    "        \"\"\"Flip image horizontally and re-serialize back as jpeg\"\"\"\n",
    "        img = Image.open(io.BytesIO(binary_content))\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # Save back as jpeg\n",
    "        output = io.BytesIO()\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "    \n",
    "    return df_series.apply(flip_image)\n",
    "\n",
    "df_flipped = (\n",
    "    df.withColumn(\"image\", flip_image_horizontally_udf(\"image\").alias('image', metadata=image_meta))\n",
    "    .withColumn(\"name\", regexp_replace(\"name\", lit(\".jpg\"), lit(\"_flipped.jpg\")))\n",
    "    .withColumn(\"path\", lit(\"n/a\") )\n",
    ")\n",
    "\n",
    "display(df_flipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a966c90e-55ed-42e1-9ca8-85e39a20b69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flipped.write.mode(\"append\").format('parquet').save(f\"{mount_path}/images_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02796453-5c91-4f88-b9e8-b1b02405035f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.read.format(\"parquet\").load(f\"{mount_path}/images_resized\")\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5269e59e-63f5-459d-887c-7d8947ff39d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "noisy_df = create_file_into_df(\"noisy_images\")\n",
    "noisy_df = noisy_df.withColumn('label', lit('noisy'))\n",
    "resized_noisy_df = (\n",
    "    noisy_df.withColumn(\"image\", resize_image_udf(\"path\"))\n",
    "            .withColumn('image',col('image').alias('image', metadata=image_meta))\n",
    ")\n",
    "display(resized_noisy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8188df-c94c-4336-abbc-6189b4ab60cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flipped_resized_noisy_df = (\n",
    "    resized_noisy_df.withColumn(\"image\", flip_image_horizontally_udf(\"image\").alias('image', metadata=image_meta))\n",
    ")\n",
    "final_noisy_df = resized_noisy_df.union(flipped_resized_noisy_df)\n",
    "display(final_noisy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93bb97a8-66d9-4471-bc17-6fff381ba604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import DataFrame\n",
    "\n",
    "\n",
    "final_df = reduce(DataFrame.unionAll, [df, df_flipped, resized_noisy_df, flipped_resized_noisy_df])\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9e1ea6-a86e-411b-bbd5-cdb89fa86991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").format('parquet').save(f\"{mount_path}/images_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec957474-344c-402c-ac5a-000112633a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    final_df.groupBy('label')\n",
    "            .count()\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed53d39-cacd-410f-95e1-f0b6d69c7c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Image Augmentation (이미지 증강)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927c4534-ce4c-4e3d-bee4-6e7f97831204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018b0be1-eb2f-4a2d-8456-eada666ffa05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(f\"{mount_path}/images_final\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd72ce4-61c7-4036-8b9c-4c1c83cdbb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49029123-fd2e-42fa-9c07-f5b2d1f636fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkZi5ncm91cEJ5KCdsYWJlbCcpLmNvdW50KCkp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewc83238e\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewc83238e\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewc83238e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewc83238e) SELECT `label`,SUM(`count`) `column_116c06d1346` FROM q GROUP BY `label`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewc83238e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "label",
             "id": "column_116c06d1345"
            },
            "y": [
             {
              "column": "count",
              "id": "column_116c06d1346",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_116c06d1346": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "15319c8a-a495-476f-baa3-15c0da706bcc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 23.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "label",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "label",
           "type": "column"
          },
          {
           "alias": "column_116c06d1346",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.groupBy('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ab850d-20f4-4bd5-b812-b5a3bf31adf9",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":155,\"path\":351},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757272894680}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "TEST: Augment Anomales"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "from PIL import Image\n",
    "from pyspark.sql.functions import pandas_udf, col, regexp_replace, lit\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def transpose_image_udf(df_series):\n",
    "    def transpose_image(content):\n",
    "        \"\"\"Transpose image and serialize back as jpeg\"\"\"\n",
    "        image = Image.open(io.BytesIO(content))\n",
    "        transpose_types = ['horizontal', 'vertical', 'rotate_90', 'rotate_180', 'rotate_270', 'squash&skew']\n",
    "\n",
    "        # Randomly selet a subset of transpose types to apply\n",
    "        selected_transpose_types = random.sample(transpose_types, random.randint(1, len(transpose_types)))\n",
    "\n",
    "        # selected_transpose_types = transpose_types[-1:]\n",
    "\n",
    "        # squash & skew matrix\n",
    "        width, height = image.size\n",
    "        ss_matrix = (1, 0.3, -width * 0.15,\n",
    "                     0.3, 1, - height * 0.15)\n",
    "\n",
    "        for transpose_type in selected_transpose_types:\n",
    "            match transpose_type:\n",
    "                case 'horizontal':\n",
    "                    image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                case 'vertical':\n",
    "                    image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                case 'rotate_90':\n",
    "                    image = image.transpose(Image.ROTATE_90)\n",
    "                case 'rotate_180':\n",
    "                    image = image.transpose(Image.ROTATE_180)\n",
    "                case 'rotate_270':\n",
    "                    image = image.transpose(Image.ROTATE_270)\n",
    "                case 'squash&skew':\n",
    "                    image = image.transform((width, height), Image.AFFINE, ss_matrix)\n",
    "        \n",
    "        # Save back as jpeg\n",
    "        output = io.BytesIO()\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "    \n",
    "    return df_series.apply(transpose_image)\n",
    "\n",
    "# Define the image metadata\n",
    "image_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\": \"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "# Apply the UDF to transpose image randomly with multiple transpose types\n",
    "noisy_df_transposed = (\n",
    "    df.filter(col('label') == 'noisy')\n",
    "    .withColumn(\"image\", transpose_image_udf(\"image\").alias('image', metadata=image_meta))\n",
    "    .withColumn('name', regexp_replace(col('name'), '.jpg', '_tr.jpg'))\n",
    "    .withColumn('path', lit('n/a'))\n",
    ")\n",
    "\n",
    "display(df.filter(col('label') == 'noisy'))\n",
    "display(noisy_df_transposed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbc8846-1ca1-4559-95f8-fdc2f3fb03e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Augment Anomalies"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import DataFrame\n",
    "\n",
    "# Generate muliple df_transposed DataFrames\n",
    "num_transposed_dfs = 5\n",
    "transposed_dfs = []\n",
    "\n",
    "for _ in range(num_transposed_dfs):\n",
    "    df_transposed = (\n",
    "        df.filter(col('label') == 'noisy')\n",
    "        .withColumn(\"image\", transpose_image_udf(\"image\").alias('image', metadata=image_meta))\n",
    "        .withColumn('name', regexp_replace(col('name'), '.jpg', '_tr.jpg'))\n",
    "        .withColumn('path', lit('n/a'))\n",
    "    )\n",
    "    transposed_dfs.append(df_transposed)\n",
    "\n",
    "noisy_df_transposed = reduce(DataFrame.union, transposed_dfs)\n",
    "display(noisy_df_transposed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90999c2-e8c0-4204-9264-ae2fc28fff61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salt and Pepper Patches"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def add_salt_pepper_patches_udf(df_series):\n",
    "    def add_salt_pepper_patches(content):\n",
    "        \"\"\"Adds an irregular, polygonal noise patchs to the image and serialize back as jpeg\"\"\"\n",
    "        patch_pixels = 500\n",
    "        noise_value = 255\n",
    "        \n",
    "        image = Image.open(io.BytesIO(content))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        width, height = image.size\n",
    "\n",
    "        # radius r\n",
    "        r = int(np.sqrt(patch_pixels) / np.pi)\n",
    "        r = max(r, 5) # 반경이 최소 5 pix 이상되어야함\n",
    "\n",
    "        # random center point for the noise patch\n",
    "        center_x = np.random.randint(r, width - r)\n",
    "        center_y = np.random.randint(r, height - r)\n",
    "        \n",
    "        num_points = np.random.randint(5,10)\n",
    "        angles = np.linspace(0, 2*np.pi, num_points, endpoint=False)\n",
    "        angles += np.random.uniform(0,2 * np.pi / num_points, size = num_points)\n",
    "        radii = np.random.uniform(0.5 * r, r * 1.5, size=num_points)\n",
    "\n",
    "        points = [\n",
    "          (int(center_x + radius * np.cos(angle)),\n",
    "           int(center_y + radius * np.sin(angle)))\n",
    "          for angle, radius in zip(angles, radii)\n",
    "        ]\n",
    "        fill_color = (noise_value,) * 3 if image.mode == 'RGB' else noise_value\n",
    "        draw.polygon(points, fill=fill_color)\n",
    "\n",
    "        output = io.BytesIO()\n",
    "        image.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "      \n",
    "    return df_series.apply(add_salt_pepper_patches)\n",
    "  \n",
    "noise_df_damaged = (\n",
    "  df.filter(col('label') == 'dog')\n",
    "  .withColumn('image', add_salt_pepper_patches_udf('image').alias('image', metadata=image_meta))\n",
    "  .withColumn('name', regexp_replace(col('name'), '.jpg', '_damaged.jpg'))\n",
    "  .withColumn('path', lit('n/a'))\n",
    ")\n",
    "\n",
    "display(noise_df_damaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e825a282-e9ad-42a4-8c85-828f39f93194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_image_content(noise_df_damaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0588c0da-037a-4134-bfb6-458d35e12e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "noisy_df_final = noisy_df_transposed.union(noise_df_damaged)\n",
    "noisy_df_final.write.mode('overwrite').format('parquet').save(f\"{mount_path}/images_noisy_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5affb3db-59c5-4a7e-b1e6-c91a54e2c505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_image_content(noisy_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e0916e4-b82d-4b1f-9519-09383810f777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Ingestion_ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
