{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc6d6fb-7252-462f-a8f9-5383cfeb246c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the mount point exists\n",
    "mount_path = \"/mnt/vision-test\"\n",
    "dbutils.fs.ls(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412d3fce-2fe4-4e8a-a59a-fcca79c880ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the directory\n",
    "# dbutils.fs.unmount(f\"/mnt/{mount_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ed14f05-ebc2-418c-b6f1-957b5b32c694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9da478-1081-4c12-b547-8fccb31af0c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Image"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of FileInfo objects to a spark DataFrame\n",
    "def create_file_into_df(source_dir:str):\n",
    "\n",
    "    files = dbutils.fs.ls(f\"{mount_path}/{source_dir}\")\n",
    "\n",
    "    file_info_list = [\n",
    "        {\n",
    "            \"path\": file.path,\n",
    "            \"name\": file.name\n",
    "        }\n",
    "        for file in files\n",
    "        ]\n",
    "\n",
    "    return spark.createDataFrame(file_info_list)\n",
    "\n",
    "file_info_df = create_file_into_df(\"images\")\n",
    "display(file_info_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd5ff68-2753-42d8-8632-73f7d402e14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the label file\n",
    "label_files = dbutils.fs.ls(f\"{mount_path}/labels\")\n",
    "label_path = label_files[0].path\n",
    "\n",
    "label_df = spark.read.csv(label_path, header=True, inferSchema=True)\n",
    "label_df = label_df.withColumnRenamed(\"image_name\", \"name\")\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "label_df = label_df.withColumn(\"name\", concat(label_df[\"name\"], lit(\".jpg\")))\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e52cfc-dcc6-40fe-a8b1-ee6e48065f35",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757161712438}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the two DataFrames\n",
    "file_info_df = file_info_df.join(label_df, on=\"name\", how=\"left\")\n",
    "display(file_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013ccb53-ed77-407d-ae72-36abd33e3fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Spark에서는 분산되어있기 때문에 collect를 사용\n",
    "\n",
    "def display_image(df, num_images:int=5):\n",
    "    images = df.take(num_images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i, row in enumerate(images):\n",
    "        img_path = row.path.replace('dbfs:/','/dbfs/')\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1,num_images,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(row.name)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_image(file_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b07678-b894-4f62-b9fd-109e1d707977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test: Crop and resize"
    }
   },
   "outputs": [],
   "source": [
    "for row in file_info_df.collect():\n",
    "    img_path = row.path.replace('dbfs:/','/dbfs/')\n",
    "    img = Image.open(img_path)\n",
    "    width, height = img.size\n",
    "    print(f\"Image: {row.name}, size: {width} x {height}\")\n",
    "    new_size = min(width, height)\n",
    "\n",
    "    # Crop 순서: LL, UL, UR, LR\n",
    "    img = img.crop(((width-new_size)/2, (height-new_size)/2, (width+new_size)/2, (height+new_size)/2))\n",
    "    print(f\"{img.size = }\")\n",
    "\n",
    "    # Resize : 256 x 256\n",
    "    img = img.resize((256, 256), Image.NEAREST)\n",
    "    print(f\"{img.size = }\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19ebac9-58d2-46d2-839c-db9bac87d667",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pandas UDF"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def resize_image_udf(df_series: pd.Series) -> pd.Series:\n",
    "    def resize_image(path):\n",
    "        \"\"\"Resize image and serialize back as jpeg\"\"\"\n",
    "        # load image\n",
    "        img_path = path.replace('dbfs:/','/dbfs/')\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        new_size = min(width, height)\n",
    "        \n",
    "        # Crop: LL, UL, UR, LR\n",
    "        img = img.crop(((width-new_size)/2, (height-new_size)/2, (width+new_size)/2, (height+new_size)/2))\n",
    "\n",
    "        # Resize\n",
    "        img = img.resize((IMAGE_SIZE,IMAGE_SIZE), Image.NEAREST)\n",
    "\n",
    "        # Save back to jpg\n",
    "        output = io.BytesIO()\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "\n",
    "    return df_series.apply(resize_image)\n",
    "\n",
    "# Add the metadata to enable the image preview\n",
    "image_meta = {\n",
    "    \"spark.contentAnnotation\":'{\"mimeType\": \"image/jpeg\"}'\n",
    "}\n",
    "\n",
    "df = (\n",
    "    file_info_df.withColumn(\"image\", resize_image_udf(\"path\")).withColumn('image',col('image').alias('image', metadata=image_meta))\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c244a14-ddef-4c84-a1ab-ec734674788e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format('parquet').save(f\"{mount_path}/images_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ae2f3d-eaf0-4a9d-9af0-3f5ed332d5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lit \n",
    "\n",
    "@pandas_udf(BinaryType())\n",
    "def flip_image_horizontally_udf(df_series):\n",
    "    def flip_image(binary_content):\n",
    "        \"\"\"Flip image horizontally and re-serialize back as jpeg\"\"\"\n",
    "        img = Image.open(io.BytesIO(binary_content))\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # Save back as jpeg\n",
    "        output = io.BytesIO()\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        return output.getvalue()\n",
    "    \n",
    "    return df_series.apply(flip_image)\n",
    "\n",
    "df_flipped = (\n",
    "    df.withColumn(\"image\", flip_image_horizontally_udf(\"image\").alias('image', metadata=image_meta))\n",
    "    .withColumn(\"name\", regexp_replace(\"name\", lit(\".jpg\"), lit(\"_flipped.jpg\")))\n",
    "    .withColumn(\"path\", lit(\"n/a\") )\n",
    ")\n",
    "\n",
    "display(df_flipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a966c90e-55ed-42e1-9ca8-85e39a20b69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flipped.write.mode(\"append\").format('parquet').save(f\"{mount_path}/images_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02796453-5c91-4f88-b9e8-b1b02405035f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.read.format(\"parquet\").load(f\"{mount_path}/images_resized\")\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5269e59e-63f5-459d-887c-7d8947ff39d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Ingestion_ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
